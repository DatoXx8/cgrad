#include <CL/cl.h>
#include <assert.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "nn.h"
#include "runtimes/cl.h"
#include "tensor.h"
#include "utils.h"

/*
 *  TODO: Make loops more abundant in the compiler simulator
 *  TODO: Support `global_size > 1`. surprisingly not trivial cuz of race conditions. I kinda think this has to be done
 * by splitting it up through multiple kernels such that I can control the snyc up. Auto detect when ops depend on each
 * other (Also make use of splitting ops up instead of doing stuff like `if(gid < x)` whenever possible)
 *  -> For now 1 kernel per loop. This could probably be optimised
 *  TODO: Make multiple cases for when `+=` can be used (if inline_num is == 1 I think it can be used otherwise I don't
 * think so)
 *  TODO: Add multi-thread c runtime
 *  TODO: Make reduce backprop real and not fake.
 *  TODO: Maybe remove explicit backprop and make autograd things.
 *  TODO: FLOP/S estimator.
 *  TODO: Update README with installation and usage guides.
 *  TODO: Make layers that generate their own weights from the input activations and meta-weights (Do this multiple
 * times?)
 *  TODO: Investigate OpenCL apparent memory leaks. Valgrind does not find memory leaks in my code but still the memory
 * usage is *super* high and seems to be rising. Also investige the OpenCL compiler being stupidly slow
 *  TODO: Make OpenCL opt-out with a -U<macro> flag (Unsure about this one cuz it makes the code very ugly)
 *  TODO: Train solely on chess960 self play. Bunch of different heads with a core net. Piece placing chess is very
 * interesting. Generate the training data with a smaller lower quality net (possibly don't even search at all and just
 * have sample from a policy generated by the net) and then train the big net by search training
 *  TODO: Make a go engine.
 *  TODO: Make string helper functions in a string.c and string.h (Should probably do this first to avoid a bunch of
 * string weirdnes when I forget to remove a `*` somewhere and have another debugging nightmare)
 *
 * perf for 1e6 (cl, c unoptimized, c optimized):
 * (37s,128s,5s) baseline
 * (37s,128s,5s) use `+=` and stuff (I think this is optimized by the compiler anyway)
 * (48s,128s,5s) split up to multiple kernels (Slow at first but allows for more work-groups -> more work-items)
 */

int main(int argc, const char **argv) {

    INIT_TIMER();
    START_TIME();

    // const uint32_t RNG = time(NULL);
    const uint32_t RNG = 0;
    printf("INFO: RNG Seed %u\n", RNG);
    srand(RNG);
    compile_e compile_type;
    if(argc != 2) {
        printf("USAGE: %s [c|cl]\n", argv[0]);
        ERROR("Program expects 2 arguments\n");
    }
    if(!strncmp(argv[1], "-cl", 3)) {
        printf("INFO: Using OpenCL\n");
        compile_type = compile_cl;
    } else if(!strncmp(argv[1], "-c", 2)) {
        printf("INFO: Not using OpenCL\n");
        compile_type = compile_none;
    } else {
        printf("USAGE: %s [c|cl]\n", argv[0]);
        ERROR("Invaling argument\n");
    }
    cl_device_id device_id;
    cl_context context;
    if(compile_type == compile_cl) {
        int err;
        device_id = cl_device_get();
        context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &err);
    } else {
        context = NULL;
    }

    const double LEARNING = 1e-2;
    const uint64_t SAMPLES = 1;
    const uint64_t LAYERS = 4;
    const uint64_t INPUT_Z = 2;
    const uint64_t INPUT_Y = 4;
    const uint64_t INPUT_X = INPUT_Y;
    layerconfig_t *layerconfig = calloc(LAYERS, sizeof(layerconfig_t));
    assert(layerconfig);
    layerconfig_t l0 = {
        .layer_type = layer_input,
        .input_z = INPUT_Z,
        .input_y = INPUT_Y,
        .input_x = INPUT_X,
    };
    layerconfig_t l1 = {
        .layer_type = layer_convolution,
        .norm_type = norm_none,
        .convolution_filters = 2,
        .convolution_kernel_size = 3,
        .convolution_kernel_stride = 1,
        .convolution_kernel_padding = 1,
        .activation_function = activation_none,
    };
    layerconfig_t l2 = {
        .layer_type = layer_split,
        .norm_type = norm_none,
        .split_filters = 2,
        .activation_function = activation_none,
    };
    layerconfig_t l3 = {
        .layer_type = layer_reduce,
        .reduce_type = layer_reduce_max,
        .reduce_kernel_size = 2,
        .reduce_kernel_stride = 1,
    };
    layerconfig_t l4 = {
        .layer_type = layer_dense,
        .norm_type = norm_none,
        .dense_output_size = 3,
        .activation_function = activation_none,
    };
    layerconfig[0] = l0;
    layerconfig[1] = l1;
    layerconfig[2] = l2;
    layerconfig[3] = l3;
    // layerconfig[4] = l4;

    neuralnet_t neuralnet = neuralnet_alloc(LAYERS, layerconfig, LEARNING, compile_type);
    tensor_t input = tensor_alloc(SAMPLES, NEURALNET_INPUT(neuralnet).activation->buffer->sze_z,
                                  NEURALNET_INPUT(neuralnet).activation->buffer->sze_y,
                                  NEURALNET_INPUT(neuralnet).activation->buffer->sze_x, context);
    tensor_t output = tensor_alloc(SAMPLES, NEURALNET_OUTPUT(neuralnet).activation->buffer->sze_z,
                                   NEURALNET_OUTPUT(neuralnet).activation->buffer->sze_y,
                                   NEURALNET_OUTPUT(neuralnet).activation->buffer->sze_x, context);
    tensor_unary_random(&output);
    tensor_realize(&output);
    tensor_unary_random(&input);
    tensor_realize(&input);
    neuralnet_random(&neuralnet);

    // for(uint64_t i = 0; i < 1e6; i++) {
    neuralnet_forward(&neuralnet, &input);
    // }
    TENSOR_PRINT_(NEURALNET_OUTPUT(neuralnet).activation);
    for(uint64_t kernel_idx = 0; kernel_idx < neuralnet.forward_cl.kernel_num; kernel_idx++) {
        printf("%s\n", neuralnet.forward_cl.kernel[kernel_idx].source);
    }

    neuralnet_free(&neuralnet);
    tensor_free(&input);
    tensor_free(&output);
    free(layerconfig);
    if(compile_type == compile_cl) {
        clReleaseDevice(device_id);
        clReleaseContext(context);
    }

    STOP_TIME();
    PRINT_TIME("main");

    return 0;
}
