#include <CL/cl.h>
#include <assert.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "nn.h"
#include "runtimes/cl.h"
#include "tensor.h"
#include "utils.h"

/*
 *  TODO: Merge all the op types to a singular enum
 *  TODO: Make assert that supports error messages
 *  TODO: Performance profiling
 *  TODO: Decide wether all the free functions should check for NULL
 *  TODO: Rewrite the compiler entirely and move that into a compiler/ directory instead of those two files
 *  TODO: Rewrite test infrastructure to automatically minify the tests (fewest ops, cut from beginning if possible
 * etc), produce logs
 *  TODO: Make nicer logging with things like %04lu and things like that
 *  TODO: Swarm testing
 *  TODO: Add multi-thread c runtime
 *  TODO: Make reduce backprop real and not fake.
 *  TODO: Maybe remove explicit backprop and make autograd things.
 *  TODO: FLOP/S estimator.
 *  TODO: Update README with installation and usage guides.
 *  TODO: Make layers that generate their own weights from the input activations and meta-weights (Do this multiple
 * times?)
 *  TODO: Investigate OpenCL apparent memory leaks. Valgrind does not find memory leaks in my code but still the memory
 * usage is *super* high and seems to be rising. Also investige the OpenCL compiler being stupidly slow
 *  TODO: Make OpenCL opt-out with a -U<macro> flag (Unsure about this one cuz it makes the code very ugly)
 *  TODO: Train solely on chess960 self play. Bunch of different heads with a core net. Piece placing chess is very
 * interesting. Generate the training data with a smaller lower quality net (possibly don't even search at all and just
 * have sample from a policy generated by the net) and then train the big net by search training
 *  TODO: Make a go engine.
 *  TODO: Make string helper functions in a string.c and string.h (Should probably do this first to avoid a bunch of
 * string weirdnes when I forget to remove a `*` somewhere and have another debugging nightmare)
 */

int main(int argc, const char **argv) {

    time_ns_store(0);

    // const uint32_t RNG = time(NULL);
    const uint32_t RNG = 0;
    printf("INFO: RNG Seed %u\n", RNG);
    srand(RNG);
    compile_e compile_type;
    if(argc != 2) {
        printf("USAGE: %s [c|cl]\n", argv[0]);
        ERROR("Program expects 2 arguments\n");
    }
    if(!strncmp(argv[1], "-cl", 3)) {
        printf("INFO: Using OpenCL\n");
        compile_type = compile_cl;
    } else if(!strncmp(argv[1], "-c", 2)) {
        printf("INFO: Not using OpenCL\n");
        compile_type = compile_none;
    } else {
        printf("USAGE: %s [c|cl]\n", argv[0]);
        ERROR("Invaling argument\n");
    }
    cl_device_id device_id;
    cl_context context;
    if(compile_type == compile_cl) {
        int err;
        device_id = cl_device_get();
        context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &err);
    } else {
        context = NULL;
    }

    const double LEARNING = 1e-2;
    const uint64_t SAMPLES = 1;
    const uint64_t LAYERS = 5;
    const uint64_t INPUT_Z = 2;
    const uint64_t INPUT_Y = 4;
    const uint64_t INPUT_X = INPUT_Y;
    layerconfig_t *layerconfig = calloc(LAYERS, sizeof(layerconfig_t));
    assert(layerconfig);
    layerconfig_t l0 = {
        .layer_type = layer_input,
        .input_z = INPUT_Z,
        .input_y = INPUT_Y,
        .input_x = INPUT_X,
    };
    layerconfig_t l1 = {
        .layer_type = layer_convolution,
        .norm_type = norm_none,
        .convolution_filters = 2,
        .convolution_kernel_size = 3,
        .convolution_kernel_stride = 1,
        .convolution_kernel_padding = 1,
        .activation_function = activation_none,
    };
    layerconfig_t l2 = {
        .layer_type = layer_split,
        .norm_type = norm_none,
        .split_filters = 2,
        .activation_function = activation_none,
    };
    layerconfig_t l3 = {
        .layer_type = layer_reduce,
        .reduce_type = layer_reduce_max,
        .reduce_kernel_size = 2,
        .reduce_kernel_stride = 1,
    };
    layerconfig_t l4 = {
        .layer_type = layer_dense,
        .norm_type = norm_none,
        .dense_output_size = 32,
        .activation_function = activation_none,
    };
    layerconfig[0] = l0;
    layerconfig[1] = l1;
    layerconfig[2] = l2;
    layerconfig[3] = l3;
    layerconfig[4] = l4;

    neuralnet_t neuralnet = neuralnet_alloc(LAYERS, layerconfig, LEARNING, compile_type);
    tensor_t input = tensor_alloc(SAMPLES, NEURALNET_INPUT(neuralnet).activation->buffer->z_sze,
                                  NEURALNET_INPUT(neuralnet).activation->buffer->y_sze,
                                  NEURALNET_INPUT(neuralnet).activation->buffer->x_sze, context);
    tensor_t output = tensor_alloc(SAMPLES, NEURALNET_OUTPUT(neuralnet).activation->buffer->z_sze,
                                   NEURALNET_OUTPUT(neuralnet).activation->buffer->y_sze,
                                   NEURALNET_OUTPUT(neuralnet).activation->buffer->x_sze, context);
    tensor_unary_random(&output);
    tensor_realize(&output);
    tensor_unary_random(&input);
    tensor_realize(&input);
    neuralnet_random(&neuralnet);

    neuralnet_forward(&neuralnet, &input);
    TENSOR_PRINT_(NEURALNET_OUTPUT(neuralnet).activation);
    for(uint64_t i = 0; i < 1e3; i++) {
        neuralnet_backward(&neuralnet, &input, &output);
        neuralnet_learn(&neuralnet);
    }
    TENSOR_PRINT_(NEURALNET_OUTPUT(neuralnet).activation);
    TENSOR_PRINT(output);
    for(uint64_t kernel_idx = 0; kernel_idx < neuralnet.forward_cl.kernel_num; kernel_idx++) {
        printf("%s\n", neuralnet.forward_cl.kernel[kernel_idx].source);
    }

    neuralnet_free(&neuralnet);
    tensor_free(&input);
    tensor_free(&output);
    free(layerconfig);
    if(compile_type == compile_cl) {
        clReleaseDevice(device_id);
        clReleaseContext(context);
    }

    PRINT_TIME(0, time_ns_load(0));

    return 0;
}
